{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is covered by the following [license](../License.ipynb)  This note must not be removed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nFFxs-gdHx9-KVk6q-hKzYM\n",
      "oc login -u kube-admin -p nFFxs-gdHx9-KVk6q-hKzYM https://api.crc.testing:6443\n"
     ]
    }
   ],
   "source": [
    "PASSWORD=$(cat ~/import/crc/login | grep -v developer | grep Password | cut -f2 -d:)\n",
    "echo $PASSWORD\n",
    "echo oc login -u kube-admin -p $PASSWORD https://api.crc.testing:6443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.130.11\n"
     ]
    }
   ],
   "source": [
    "crc ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT=k8s-sec-2021-07-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using project \"k8s-sec-2021-07-15\" on server \"https://api.crc.testing:6443\".\n",
      "\n",
      "You can add applications to this project with the 'new-app' command. For example, try:\n",
      "\n",
      "    oc new-app rails-postgresql-example\n",
      "\n",
      "to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:\n",
      "\n",
      "    kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oc new-project $PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project \"k8s-sec-2021-07-15\" on server \"https://api.crc.testing:6443\".\n"
     ]
    }
   ],
   "source": [
    "oc project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resources found in k8s-sec-2021-07-15 namespace.\n"
     ]
    }
   ],
   "source": [
    "oc get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/nginx-unprivileged unchanged\n"
     ]
    }
   ],
   "source": [
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx-unprivileged\n",
    "  labels:\n",
    "    app: nginx-unprivileged\n",
    "spec:\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginxinc/nginx-unprivileged\n",
    "    ports:\n",
    "      - containerPort: 8080\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 READY   STATUS    RESTARTS   AGE\n",
      "nginx-unprivileged   1/1     Running   0          48s\n"
     ]
    }
   ],
   "source": [
    "oc get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Account Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem     1K-blocks     Used Available Use% Mounted on\n",
      "overlay         31970284 16014888  15955396  51% /\n",
      "tmpfs              65536        0     65536   0% /dev\n",
      "tmpfs            4592732        0   4592732   0% /sys/fs/cgroup\n",
      "shm                65536        0     65536   0% /dev/shm\n",
      "tmpfs            4592732    14424   4578308   1% /etc/hostname\n",
      "/dev/vda4       31970284 16014888  15955396  51% /etc/hosts\n",
      "\u001b[01;31m\u001b[Ktmpfs            4592732       28   4592704   1% /run/secrets/kubernetes.io/serviceaccount\u001b[m\u001b[K\n",
      "tmpfs            4592732        0   4592732   0% /proc/acpi\n",
      "tmpfs            4592732        0   4592732   0% /proc/scsi\n",
      "tmpfs            4592732        0   4592732   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- df | grep -E '(^|.*serviceaccount)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca.crt\n",
      "namespace\n",
      "service-ca.crt\n",
      "token\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- ls /run/secrets/kubernetes.io/serviceaccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhbGciOiJSUzI1NiIsImtpZCI6IkpUbVRwczhHVWhoT3otZURxcEZTVUJvZ2tJNENlYlZXV3dqUXpXTXNsRk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrOHMtc2VjLTIwMjEtMDctMTUiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC10b2tlbi1tY21jYyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYzJmNWMzNTAtZDA1Zi00YWIyLWI0NTUtMzlkODhmNDJkZGM1Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Oms4cy1zZWMtMjAyMS0wNy0xNTpkZWZhdWx0In0.gNaJ_k1xcxY8oNQ3RKCLF0m72pt4YoTsIrCDfzomZaRQ_nTRUuO_I5bF5iSbZ-Vl-ephoHCRlsAV-ExFTA2o6C3HTwwMBxAOagtfzxX9On3ySfMh754YMY2uAj3pqwgMVfgYbKYj14jLHuJTTN4VmQr-T4cRpKOUuBoiWl_jfjpaQPXbhUd4X0GLRxZzKb59Vleq0MnltjSbeoDW_dIsPGjLc1Apj9h-TODFStdZt_5m5e4oQKI3TIO4t7zL9Y_gXQ29iII-PxFDkRUxqUDHj-7xmxW2fScUIPREmyUKmBpbM6r5lgo9-X1ZEUKDln1WfKXMqUDGn-UWOcayLGJcSg"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- cat /run/secrets/kubernetes.io/serviceaccount/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid=101(nginx) gid=101(nginx) groups=101(nginx)\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "<title>Welcome to nginx!</title>\n",
      "<style>\n",
      "    body {\n",
      "        width: 35em;\n",
      "        margin: 0 auto;\n",
      "        font-family: Tahoma, Verdana, Arial, sans-serif;\n",
      "    }\n",
      "</style>\n",
      "</head>\n",
      "<body>\n",
      "<h1>Welcome to nginx!</h1>\n",
      "<p>If you see this page, the nginx web server is successfully installed and\n",
      "working. Further configuration is required.</p>\n",
      "\n",
      "<p>For online documentation and support please refer to\n",
      "<a href=\"http://nginx.org/\">nginx.org</a>.<br/>\n",
      "Commercial support is available at\n",
      "<a href=\"http://nginx.com/\">nginx.com</a>.</p>\n",
      "\n",
      "<p><em>Thank you for using nginx.</em></p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- curl -qs localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: line 0: cd: /nonexistent: No such file or directory\n",
      "command terminated with exit code 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- bash -c 'cd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We cannot write to a nonexisting home\n",
    "- that is fine\n",
    "- but wait, what about ```/tmp```\n",
    "# **We can install an executable** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   154  100   154    0     0    590      0 --:--:-- --:--:-- --:--:--   590\n",
      "100 38.3M  100 38.3M    0     0  14.6M      0  0:00:02  0:00:02 --:--:-- 20.6M\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged --  curl -L -o /tmp/kubectl https://dl.k8s.io/release/v1.20.0/bin/linux/amd64/kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 39292\n",
      "dr-xr-xr-x. 1 root  root        39 Jul 15 14:14 ../\n",
      "drwx------. 2 nginx nginx        6 Jul 15 14:14 uwsgi_temp/\n",
      "drwx------. 2 nginx nginx        6 Jul 15 14:14 scgi_temp/\n",
      "drwx------. 2 nginx nginx        6 Jul 15 14:14 proxy_temp/\n",
      "drwx------. 2 nginx nginx        6 Jul 15 14:14 fastcgi_temp/\n",
      "drwx------. 2 nginx nginx        6 Jul 15 14:14 client_temp/\n",
      "-rw-r--r--. 1 nginx nginx        2 Jul 15 14:14 nginx.pid\n",
      "drwxrwxrwt. 1 root  root       130 Jul 15 14:17 ./\n",
      "-rwxr-xr-x. 1 nginx nginx 40230912 Jul 15 14:17 kubectl*\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- chmod goa+x /tmp/kubectl\n",
    "oc exec nginx-unprivileged -- ls -alFtr /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kubectl controls the Kubernetes cluster manager.\n",
      "\n",
      " Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/\n",
      "\n",
      "Basic Commands (Beginner):\n",
      "  create        Create a resource from a file or from stdin.\n",
      "  expose        Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service\n",
      "  run           Run a particular image on the cluster\n",
      "  set           Set specific features on objects\n",
      "\n",
      "Basic Commands (Intermediate):\n",
      "  explain       Documentation of resources\n",
      "  get           Display one or many resources\n",
      "  edit          Edit a resource on the server\n",
      "  delete        Delete resources by filenames, stdin, resources and names, or by resources and label selector\n",
      "\n",
      "Deploy Commands:\n",
      "  rollout       Manage the rollout of a resource\n",
      "  scale         Set a new size for a Deployment, ReplicaSet or Replication Controller\n",
      "  autoscale     Auto-scale a Deployment, ReplicaSet, or ReplicationController\n",
      "\n",
      "Cluster Management Commands:\n",
      "  certificate   Modify certificate resources.\n",
      "  cluster-info  Display cluster info\n",
      "  top           Display Resource (CPU/Memory/Storage) usage.\n",
      "  cordon        Mark node as unschedulable\n",
      "  uncordon      Mark node as schedulable\n",
      "  drain         Drain node in preparation for maintenance\n",
      "  taint         Update the taints on one or more nodes\n",
      "\n",
      "Troubleshooting and Debugging Commands:\n",
      "  describe      Show details of a specific resource or group of resources\n",
      "  logs          Print the logs for a container in a pod\n",
      "  attach        Attach to a running container\n",
      "  exec          Execute a command in a container\n",
      "  port-forward  Forward one or more local ports to a pod\n",
      "  proxy         Run a proxy to the Kubernetes API server\n",
      "  cp            Copy files and directories to and from containers.\n",
      "  auth          Inspect authorization\n",
      "  debug         Create debugging sessions for troubleshooting workloads and nodes\n",
      "\n",
      "Advanced Commands:\n",
      "  diff          Diff live version against would-be applied version\n",
      "  apply         Apply a configuration to a resource by filename or stdin\n",
      "  patch         Update field(s) of a resource\n",
      "  replace       Replace a resource by filename or stdin\n",
      "  wait          Experimental: Wait for a specific condition on one or many resources.\n",
      "  kustomize     Build a kustomization target from a directory or a remote url.\n",
      "\n",
      "Settings Commands:\n",
      "  label         Update the labels on a resource\n",
      "  annotate      Update the annotations on a resource\n",
      "  completion    Output shell completion code for the specified shell (bash or zsh)\n",
      "\n",
      "Other Commands:\n",
      "  api-resources Print the supported API resources on the server\n",
      "  api-versions  Print the supported API versions on the server, in the form of \"group/version\"\n",
      "  config        Modify kubeconfig files\n",
      "  plugin        Provides utilities for interacting with plugins.\n",
      "  version       Print the client and server version information\n",
      "\n",
      "Usage:\n",
      "  kubectl [flags] [options]\n",
      "\n",
      "Use \"kubectl <command> --help\" for more information about a given command.\n",
      "Use \"kubectl options\" for a list of global command-line options (applies to all commands).\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- /tmp/kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have a token, but it is useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0715 14:17:30.927546     114 request.go:655] Throttling request took 1.166762115s, request: GET:https://10.217.4.1:443/apis/rbac.authorization.k8s.io/v1?timeout=32s\n",
      "I0715 14:17:41.127570     114 request.go:655] Throttling request took 11.36485421s, request: GET:https://10.217.4.1:443/apis/controlplane.operator.openshift.io/v1alpha1?timeout=32s\n",
      "Error from server (Forbidden): pods is forbidden: User \"system:serviceaccount:k8s-sec-2021-07-15:default\" cannot list resource \"pods\" in API group \"\" in the namespace \"k8s-sec-2021-07-15\"\n",
      "command terminated with exit code 1\n",
      "\n",
      "\n",
      "FAILED\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- /tmp/kubectl get pods || (echo; echo; echo \"FAILED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc get scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"elastic\" already exists with the same configuration, skipping\n"
     ]
    }
   ],
   "source": [
    "helm repo add elastic https://helm.elastic.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "# Source: elasticsearch/templates/poddisruptionbudget.yaml\n",
      "apiVersion: policy/v1beta1\n",
      "kind: PodDisruptionBudget\n",
      "metadata:\n",
      "  name: \"elasticsearch-master-pdb\"\n",
      "spec:\n",
      "  maxUnavailable: 1\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: \"elasticsearch-master\"\n",
      "---\n",
      "# Source: elasticsearch/templates/service.yaml\n",
      "kind: Service\n",
      "apiVersion: v1\n",
      "metadata:\n",
      "  name: elasticsearch-master\n",
      "  labels:\n",
      "    heritage: \"Helm\"\n",
      "    release: \"elasticsearch\"\n",
      "    chart: \"elasticsearch\"\n",
      "    app: \"elasticsearch-master\"\n",
      "  annotations:\n",
      "    {}\n",
      "spec:\n",
      "  type: ClusterIP\n",
      "  selector:\n",
      "    release: \"elasticsearch\"\n",
      "    chart: \"elasticsearch\"\n",
      "    app: \"elasticsearch-master\"\n",
      "  ports:\n",
      "  - name: http\n",
      "    protocol: TCP\n",
      "    port: 9200\n",
      "  - name: transport\n",
      "    protocol: TCP\n",
      "    port: 9300\n",
      "---\n",
      "# Source: elasticsearch/templates/service.yaml\n",
      "kind: Service\n",
      "apiVersion: v1\n",
      "metadata:\n",
      "  name: elasticsearch-master-headless\n",
      "  labels:\n",
      "    heritage: \"Helm\"\n",
      "    release: \"elasticsearch\"\n",
      "    chart: \"elasticsearch\"\n",
      "    app: \"elasticsearch-master\"\n",
      "  annotations:\n",
      "    service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"\n",
      "spec:\n",
      "  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve\n",
      "  # Create endpoints also if the related pod isn't ready\n",
      "  publishNotReadyAddresses: true\n",
      "  selector:\n",
      "    app: \"elasticsearch-master\"\n",
      "  ports:\n",
      "  - name: http\n",
      "    port: 9200\n",
      "  - name: transport\n",
      "    port: 9300\n",
      "---\n",
      "# Source: elasticsearch/templates/statefulset.yaml\n",
      "apiVersion: apps/v1\n",
      "kind: StatefulSet\n",
      "metadata:\n",
      "  name: elasticsearch-master\n",
      "  labels:\n",
      "    heritage: \"Helm\"\n",
      "    release: \"elasticsearch\"\n",
      "    chart: \"elasticsearch\"\n",
      "    app: \"elasticsearch-master\"\n",
      "  annotations:\n",
      "    esMajorVersion: \"7\"\n",
      "spec:\n",
      "  serviceName: elasticsearch-master-headless\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: \"elasticsearch-master\"\n",
      "  replicas: 3\n",
      "  podManagementPolicy: Parallel\n",
      "  updateStrategy:\n",
      "    type: RollingUpdate\n",
      "  volumeClaimTemplates:\n",
      "  - metadata:\n",
      "      name: elasticsearch-master\n",
      "    spec:\n",
      "      accessModes:\n",
      "      - ReadWriteOnce\n",
      "      resources:\n",
      "        requests:\n",
      "          storage: 30Gi\n",
      "  template:\n",
      "    metadata:\n",
      "      name: \"elasticsearch-master\"\n",
      "      labels:\n",
      "        release: \"elasticsearch\"\n",
      "        chart: \"elasticsearch\"\n",
      "        app: \"elasticsearch-master\"\n",
      "      annotations:\n",
      "        \n",
      "    spec:\n",
      "      securityContext:\n",
      "        fsGroup: 1000\n",
      "        runAsUser: 1000\n",
      "      affinity:\n",
      "        podAntiAffinity:\n",
      "          requiredDuringSchedulingIgnoredDuringExecution:\n",
      "          - labelSelector:\n",
      "              matchExpressions:\n",
      "              - key: app\n",
      "                operator: In\n",
      "                values:\n",
      "                - \"elasticsearch-master\"\n",
      "            topologyKey: kubernetes.io/hostname\n",
      "      terminationGracePeriodSeconds: 120\n",
      "      volumes:\n",
      "      enableServiceLinks: true\n",
      "      \u001b[01;31m\u001b[KinitContainers\u001b[m\u001b[K:\n",
      "      - name: configure-sysctl\n",
      "        securityContext:\n",
      "          runAsUser: 0\n",
      "          privileged: true\n",
      "        image: \"docker.elastic.co/elasticsearch/elasticsearch:7.13.2\"\n",
      "        imagePullPolicy: \"IfNotPresent\"\n",
      "        command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"]\n",
      "        resources:\n",
      "          {}\n",
      "\n",
      "      containers:\n",
      "      - name: \"elasticsearch\"\n",
      "        securityContext:\n",
      "          capabilities:\n",
      "            drop:\n",
      "            - ALL\n",
      "          runAsNonRoot: true\n",
      "          runAsUser: 1000\n",
      "        image: \"docker.elastic.co/elasticsearch/elasticsearch:7.13.2\"\n",
      "        imagePullPolicy: \"IfNotPresent\"\n",
      "        readinessProbe:\n",
      "          exec:\n",
      "            command:\n",
      "              - sh\n",
      "              - -c\n",
      "              - |\n",
      "                #!/usr/bin/env bash -e\n",
      "                # If the node is starting up wait for the cluster to be ready (request params: \"wait_for_status=green&timeout=1s\" )\n",
      "                # Once it has started only check that the node itself is responding\n",
      "                START_FILE=/tmp/.es_start_file\n",
      "\n",
      "                # Disable nss cache to avoid filling dentry cache when calling curl\n",
      "                # This is required with Elasticsearch Docker using nss < 3.52\n",
      "                export NSS_SDB_USE_CACHE=no\n",
      "\n",
      "                http () {\n",
      "                  local path=\"${1}\"\n",
      "                  local args=\"${2}\"\n",
      "                  set -- -XGET -s\n",
      "\n",
      "                  if [ \"$args\" != \"\" ]; then\n",
      "                    set -- \"$@\" $args\n",
      "                  fi\n",
      "\n",
      "                  if [ -n \"${ELASTIC_USERNAME}\" ] && [ -n \"${ELASTIC_PASSWORD}\" ]; then\n",
      "                    set -- \"$@\" -u \"${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}\"\n",
      "                  fi\n",
      "\n",
      "                  curl --output /dev/null -k \"$@\" \"http://127.0.0.1:9200${path}\"\n",
      "                }\n",
      "\n",
      "                if [ -f \"${START_FILE}\" ]; then\n",
      "                  echo 'Elasticsearch is already running, lets check the node is healthy'\n",
      "                  HTTP_CODE=$(http \"/\" \"-w %{http_code}\")\n",
      "                  RC=$?\n",
      "                  if [[ ${RC} -ne 0 ]]; then\n",
      "                    echo \"curl --output /dev/null -k -XGET -s -w '%{http_code}' \\${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}\"\n",
      "                    exit ${RC}\n",
      "                  fi\n",
      "                  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x\n",
      "                  if [[ ${HTTP_CODE} == \"200\" ]]; then\n",
      "                    exit 0\n",
      "                  elif [[ ${HTTP_CODE} == \"503\" && \"7\" == \"6\" ]]; then\n",
      "                    exit 0\n",
      "                  else\n",
      "                    echo \"curl --output /dev/null -k -XGET -s -w '%{http_code}' \\${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}\"\n",
      "                    exit 1\n",
      "                  fi\n",
      "\n",
      "                else\n",
      "                  echo 'Waiting for elasticsearch cluster to become ready (request params: \"wait_for_status=green&timeout=1s\" )'\n",
      "                  if http \"/_cluster/health?wait_for_status=green&timeout=1s\" \"--fail\" ; then\n",
      "                    touch ${START_FILE}\n",
      "                    exit 0\n",
      "                  else\n",
      "                    echo 'Cluster is not yet ready (request params: \"wait_for_status=green&timeout=1s\" )'\n",
      "                    exit 1\n",
      "                  fi\n",
      "                fi\n",
      "          failureThreshold: 3\n",
      "          initialDelaySeconds: 10\n",
      "          periodSeconds: 10\n",
      "          successThreshold: 3\n",
      "          timeoutSeconds: 5\n",
      "        ports:\n",
      "        - name: http\n",
      "          containerPort: 9200\n",
      "        - name: transport\n",
      "          containerPort: 9300\n",
      "        resources:\n",
      "          limits:\n",
      "            cpu: 1000m\n",
      "            memory: 2Gi\n",
      "          requests:\n",
      "            cpu: 1000m\n",
      "            memory: 2Gi\n",
      "        env:\n",
      "          - name: node.name\n",
      "            valueFrom:\n",
      "              fieldRef:\n",
      "                fieldPath: metadata.name\n",
      "          - name: cluster.initial_master_nodes\n",
      "            value: \"elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\"\n",
      "          - name: discovery.seed_hosts\n",
      "            value: \"elasticsearch-master-headless\"\n",
      "          - name: cluster.name\n",
      "            value: \"elasticsearch\"\n",
      "          - name: network.host\n",
      "            value: \"0.0.0.0\"\n",
      "          - name: node.data\n",
      "            value: \"true\"\n",
      "          - name: node.ingest\n",
      "            value: \"true\"\n",
      "          - name: node.master\n",
      "            value: \"true\"\n",
      "          - name: node.ml\n",
      "            value: \"true\"\n",
      "          - name: node.remote_cluster_client\n",
      "            value: \"true\"\n",
      "        volumeMounts:\n",
      "          - name: \"elasticsearch-master\"\n",
      "            mountPath: /usr/share/elasticsearch/data\n",
      "---\n",
      "# Source: elasticsearch/templates/test/test-elasticsearch-health.yaml\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: \"elasticsearch-ppbef-test\"\n",
      "  annotations:\n",
      "    \"helm.sh/hook\": test\n",
      "    \"helm.sh/hook-delete-policy\": hook-succeeded\n",
      "spec:\n",
      "  securityContext:\n",
      "    fsGroup: 1000\n",
      "    runAsUser: 1000\n",
      "  containers:\n",
      "  - name: \"elasticsearch-zcymo-test\"\n",
      "    image: \"docker.elastic.co/elasticsearch/elasticsearch:7.13.2\"\n",
      "    imagePullPolicy: \"IfNotPresent\"\n",
      "    command:\n",
      "      - \"sh\"\n",
      "      - \"-c\"\n",
      "      - |\n",
      "        #!/usr/bin/env bash -e\n",
      "        curl -XGET --fail 'elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s'\n",
      "  restartPolicy: Never\n"
     ]
    }
   ],
   "source": [
    "helm template elasticsearch elastic/elasticsearch | grep -A 10 -E '(^|initContainers)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \u001b[01;31m\u001b[KinitContainers\u001b[m\u001b[K:\n",
      "\u001b[01;31m\u001b[K      - name: configure-sysctl\u001b[m\u001b[K\n",
      "        securityContext:\n",
      "          runAsUser: 0\n",
      "          privileged: true\n",
      "        image: \"docker.elastic.co/elasticsearch/elasticsearch:7.13.2\"\n",
      "        imagePullPolicy: \"IfNotPresent\"\n",
      "\u001b[01;31m\u001b[K        command: [\"sysctl\", \"-w\", \"vm.max_map_count=262144\"]\u001b[m\u001b[K\n",
      "        resources:\n",
      "          {}\n",
      "\n",
      "      containers:\n",
      "      - name: \"elasticsearch\"\n",
      "        securityContext:\n",
      "          capabilities:\n",
      "            drop:\n",
      "            - ALL\n",
      "          runAsNonRoot: true\n"
     ]
    }
   ],
   "source": [
    "helm template elasticsearch elastic/elasticsearch | grep -A 10 -E '(initContainers|.*sysctl.*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: elasticsearch\n",
      "LAST DEPLOYED: Thu Jul 15 16:19:22 2021\n",
      "NAMESPACE: k8s-sec-2021-07-15\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "NOTES:\n",
      "1. Watch all cluster members come up.\n",
      "  $ kubectl get pods --namespace=k8s-sec-2021-07-15 -l app=elasticsearch-master -w\n",
      "2. Test cluster health using Helm test.\n",
      "  $ helm test elasticsearch\n"
     ]
    }
   ],
   "source": [
    "helm install elasticsearch elastic/elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY   STATUS    RESTARTS   AGE\n",
      "pod/nginx-unprivileged   1/1     Running   0          4m38s\n",
      "\n",
      "NAME                                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n",
      "service/elasticsearch-master            ClusterIP   10.217.4.102   <none>        9200/TCP,9300/TCP   5s\n",
      "service/elasticsearch-master-headless   ClusterIP   None           <none>        9200/TCP,9300/TCP   5s\n",
      "\n",
      "NAME                                    READY   AGE\n",
      "statefulset.apps/elasticsearch-master   0/3     5s\n"
     ]
    }
   ],
   "source": [
    "oc get all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               elasticsearch-master\n",
      "Namespace:          k8s-sec-2021-07-15\n",
      "CreationTimestamp:  Thu, 15 Jul 2021 16:19:22 +0200\n",
      "Selector:           app=elasticsearch-master\n",
      "Labels:             app=elasticsearch-master\n",
      "                    app.kubernetes.io/managed-by=Helm\n",
      "                    chart=elasticsearch\n",
      "                    heritage=Helm\n",
      "                    release=elasticsearch\n",
      "Annotations:        esMajorVersion: 7\n",
      "                    meta.helm.sh/release-name: elasticsearch\n",
      "                    meta.helm.sh/release-namespace: k8s-sec-2021-07-15\n",
      "Replicas:           3 desired | 0 total\n",
      "Update Strategy:    RollingUpdate\n",
      "Pods Status:        0 Running / 0 Waiting / 0 Succeeded / 0 Failed\n",
      "Pod Template:\n",
      "  Labels:  app=elasticsearch-master\n",
      "           chart=elasticsearch\n",
      "           release=elasticsearch\n",
      "  Init Containers:\n",
      "   configure-sysctl:\n",
      "    Image:      docker.elastic.co/elasticsearch/elasticsearch:7.13.2\n",
      "    Port:       <none>\n",
      "    Host Port:  <none>\n",
      "    Command:\n",
      "      sysctl\n",
      "      -w\n",
      "      vm.max_map_count=262144\n",
      "    Environment:  <none>\n",
      "    Mounts:       <none>\n",
      "  Containers:\n",
      "   elasticsearch:\n",
      "    Image:       docker.elastic.co/elasticsearch/elasticsearch:7.13.2\n",
      "    Ports:       9200/TCP, 9300/TCP\n",
      "    Host Ports:  0/TCP, 0/TCP\n",
      "    Limits:\n",
      "      cpu:     1\n",
      "      memory:  2Gi\n",
      "    Requests:\n",
      "      cpu:      1\n",
      "      memory:   2Gi\n",
      "    Readiness:  exec [sh -c #!/usr/bin/env bash -e\n",
      "# If the node is starting up wait for the cluster to be ready (request params: \"wait_for_status=green&timeout=1s\" )\n",
      "# Once it has started only check that the node itself is responding\n",
      "START_FILE=/tmp/.es_start_file\n",
      "\n",
      "# Disable nss cache to avoid filling dentry cache when calling curl\n",
      "# This is required with Elasticsearch Docker using nss < 3.52\n",
      "export NSS_SDB_USE_CACHE=no\n",
      "\n",
      "http () {\n",
      "  local path=\"${1}\"\n",
      "  local args=\"${2}\"\n",
      "  set -- -XGET -s\n",
      "\n",
      "  if [ \"$args\" != \"\" ]; then\n",
      "    set -- \"$@\" $args\n",
      "  fi\n",
      "\n",
      "  if [ -n \"${ELASTIC_USERNAME}\" ] && [ -n \"${ELASTIC_PASSWORD}\" ]; then\n",
      "    set -- \"$@\" -u \"${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}\"\n",
      "  fi\n",
      "\n",
      "  curl --output /dev/null -k \"$@\" \"http://127.0.0.1:9200${path}\"\n",
      "}\n",
      "\n",
      "if [ -f \"${START_FILE}\" ]; then\n",
      "  echo 'Elasticsearch is already running, lets check the node is healthy'\n",
      "  HTTP_CODE=$(http \"/\" \"-w %{http_code}\")\n",
      "  RC=$?\n",
      "  if [[ ${RC} -ne 0 ]]; then\n",
      "    echo \"curl --output /dev/null -k -XGET -s -w '%{http_code}' \\${BASIC_AUTH} http://127.0.0.1:9200/ failed with RC ${RC}\"\n",
      "    exit ${RC}\n",
      "  fi\n",
      "  # ready if HTTP code 200, 503 is tolerable if ES version is 6.x\n",
      "  if [[ ${HTTP_CODE} == \"200\" ]]; then\n",
      "    exit 0\n",
      "  elif [[ ${HTTP_CODE} == \"503\" && \"7\" == \"6\" ]]; then\n",
      "    exit 0\n",
      "  else\n",
      "    echo \"curl --output /dev/null -k -XGET -s -w '%{http_code}' \\${BASIC_AUTH} http://127.0.0.1:9200/ failed with HTTP code ${HTTP_CODE}\"\n",
      "    exit 1\n",
      "  fi\n",
      "\n",
      "else\n",
      "  echo 'Waiting for elasticsearch cluster to become ready (request params: \"wait_for_status=green&timeout=1s\" )'\n",
      "  if http \"/_cluster/health?wait_for_status=green&timeout=1s\" \"--fail\" ; then\n",
      "    touch ${START_FILE}\n",
      "    exit 0\n",
      "  else\n",
      "    echo 'Cluster is not yet ready (request params: \"wait_for_status=green&timeout=1s\" )'\n",
      "    exit 1\n",
      "  fi\n",
      "fi\n",
      "] delay=10s timeout=5s period=10s #success=3 #failure=3\n",
      "    Environment:\n",
      "      node.name:                      (v1:metadata.name)\n",
      "      cluster.initial_master_nodes:  elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n",
      "      discovery.seed_hosts:          elasticsearch-master-headless\n",
      "      cluster.name:                  elasticsearch\n",
      "      network.host:                  0.0.0.0\n",
      "      node.data:                     true\n",
      "      node.ingest:                   true\n",
      "      node.master:                   true\n",
      "      node.ml:                       true\n",
      "      node.remote_cluster_client:    true\n",
      "    Mounts:\n",
      "      /usr/share/elasticsearch/data from elasticsearch-master (rw)\n",
      "  Volumes:  <none>\n",
      "Volume Claims:\n",
      "  Name:          elasticsearch-master\n",
      "  StorageClass:  \n",
      "  Labels:        <none>\n",
      "  Annotations:   <none>\n",
      "  Capacity:      30Gi\n",
      "  Access Modes:  [ReadWriteOnce]\n",
      "Events:\n",
      "  Type     Reason            Age                From                    Message\n",
      "  ----     ------            ----               ----                    -------\n",
      "  Normal   SuccessfulCreate  12s                statefulset-controller  create Claim elasticsearch-master-elasticsearch-master-0 Pod elasticsearch-master-0 in StatefulSet elasticsearch-master success\n",
      "  Warning  \u001b[01;31m\u001b[KFailedCreate\u001b[m\u001b[K      2s (x12 over 12s)  statefulset-controller  create Pod elasticsearch-master-0 in StatefulSet elasticsearch-master failed error: pods \"elasticsearch-master-0\" is forbidden: unable to validate against any security context constraint: [provider restricted: .spec.securityContext.fsGroup: Invalid value: []int64{1000}: 1000 is not an allowed group spec.initContainers[0].securityContext.runAsUser: Invalid value: 0: must be in the ranges: [1000620000, 1000629999] spec.initContainers[0].\u001b[01;31m\u001b[KsecurityContext.privileged\u001b[m\u001b[K: Invalid value: true: Privileged containers are not allowed spec.containers[0].securityContext.runAsUser: Invalid value: 1000: must be in the ranges: [1000620000, 1000629999]]\n"
     ]
    }
   ],
   "source": [
    "oc describe statefulset.apps/elasticsearch-master | grep -E '(^|FailedCreate|securityContext.privileged)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY   STATUS    RESTARTS   AGE\n",
      "pod/nginx-unprivileged   1/1     Running   0          5m4s\n",
      "\n",
      "NAME                                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n",
      "service/elasticsearch-master            ClusterIP   10.217.4.102   <none>        9200/TCP,9300/TCP   31s\n",
      "service/elasticsearch-master-headless   ClusterIP   None           <none>        9200/TCP,9300/TCP   31s\n",
      "\n",
      "NAME                                    READY   AGE\n",
      "statefulset.apps/elasticsearch-master   0/3     31s\n"
     ]
    }
   ],
   "source": [
    "oc get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I want Elasticsearch running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         \tNAMESPACE         \tREVISION\tUPDATED                                 \tSTATUS  \tCHART               \tAPP VERSION\n",
      "elasticsearch\tk8s-sec-2021-07-15\t1       \t2021-07-15 16:19:22.157997937 +0200 CEST\tdeployed\telasticsearch-7.13.2\t7.13.2     \n"
     ]
    }
   ],
   "source": [
    "helm ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google, Google, how to start an Elasticsearch cluster\n",
    "\n",
    "**Cool, we have a an advice like this from a vendor**\n",
    "\n",
    "_If you are using GKE, make sure your user has cluster-admin permissions. For more information, see Prerequisites for using Kubernetes RBAC on GKE._\n",
    "\n",
    "\n",
    "https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-deploy-eck.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-binding-k8s-sec-2021-07-15 created\n"
     ]
    }
   ],
   "source": [
    "kubectl create clusterrolebinding cluster-admin-binding-$PROJECT --clusterrole cluster-admin --serviceaccount $PROJECT:default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"elasticsearch\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "helm uninstall elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME: elasticsearch\n",
      "LAST DEPLOYED: Thu Jul 15 16:21:30 2021\n",
      "NAMESPACE: k8s-sec-2021-07-15\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "NOTES:\n",
      "1. Watch all cluster members come up.\n",
      "  $ kubectl get pods --namespace=k8s-sec-2021-07-15 -l app=elasticsearch-master -w\n",
      "2. Test cluster health using Helm test.\n",
      "  $ helm test elasticsearch\n"
     ]
    }
   ],
   "source": [
    "helm install elasticsearch elastic/elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                         READY   STATUS    RESTARTS   AGE\n",
      "pod/elasticsearch-master-0   0/1     Running   0          5s\n",
      "pod/elasticsearch-master-1   0/1     Pending   0          5s\n",
      "pod/elasticsearch-master-2   0/1     Pending   0          5s\n",
      "pod/nginx-unprivileged       1/1     Running   0          6m46s\n",
      "\n",
      "NAME                                    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)             AGE\n",
      "service/elasticsearch-master            ClusterIP   10.217.5.37   <none>        9200/TCP,9300/TCP   5s\n",
      "service/elasticsearch-master-headless   ClusterIP   None          <none>        9200/TCP,9300/TCP   5s\n",
      "\n",
      "NAME                                    READY   AGE\n",
      "statefulset.apps/elasticsearch-master   0/3     5s\n"
     ]
    }
   ],
   "source": [
    "oc get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Collateral Damage** \n",
    "\n",
    "in the ```nginx``` pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0715 14:21:57.562934     127 request.go:655] Throttling request took 1.159078178s, request: GET:https://10.217.4.1:443/apis/autoscaling.openshift.io/v1?timeout=32s\n",
      "I0715 14:22:07.754808     127 request.go:655] Throttling request took 11.347736128s, request: GET:https://10.217.4.1:443/apis/apps.openshift.io/v1?timeout=32s\n",
      "NAME                     READY   STATUS    RESTARTS   AGE\n",
      "elasticsearch-master-0   0/1     Running   0          38s\n",
      "elasticsearch-master-1   0/1     Pending   0          38s\n",
      "elasticsearch-master-2   0/1     Pending   0          38s\n",
      "nginx-unprivileged       1/1     Running   0          7m19s\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- /tmp/kubectl get pods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod/nginx-hacked created\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- /tmp/kubectl run nginx-hacked --image=nginxinc/nginx-unprivileged --port=8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY   STATUS    RESTARTS   AGE\n",
      "elasticsearch-master-0   0/1     Running   0          69s\n",
      "elasticsearch-master-1   0/1     Pending   0          69s\n",
      "elasticsearch-master-2   0/1     Pending   0          69s\n",
      "nginx-hacked             1/1     Running   0          9s\n",
      "nginx-unprivileged       1/1     Running   0          7m50s\n"
     ]
    }
   ],
   "source": [
    "oc get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0715 14:23:03.399102     153 request.go:655] Throttling request took 1.168316333s, request: GET:https://10.217.4.1:443/apis/policy/v1beta1?timeout=32s\n",
      "I0715 14:23:13.399327     153 request.go:655] Throttling request took 11.167293528s, request: GET:https://10.217.4.1:443/apis/monitoring.coreos.com/v1?timeout=32s\n",
      "NAMESPACE                                    NAME                                                    READY   STATUS      RESTARTS   AGE\n",
      "k8s-sec-2021-07-15                           elasticsearch-master-0                                  0/1     Running     0          104s\n",
      "k8s-sec-2021-07-15                           elasticsearch-master-1                                  0/1     Pending     0          104s\n",
      "k8s-sec-2021-07-15                           elasticsearch-master-2                                  0/1     Pending     0          104s\n",
      "k8s-sec-2021-07-15                           nginx-hacked                                            1/1     Running     0          44s\n",
      "k8s-sec-2021-07-15                           nginx-unprivileged                                      1/1     Running     0          8m25s\n",
      "openshift-apiserver-operator                 openshift-apiserver-operator-55f9bdf858-c2xfd           1/1     Running     0          34d\n",
      "openshift-apiserver                          apiserver-546d7d57dc-7fqbp                              2/2     Running     0          2d4h\n",
      "openshift-authentication-operator            authentication-operator-7c9f545b4f-28gj5                1/1     Running     0          34d\n",
      "openshift-authentication                     oauth-openshift-84d9db6f4f-9tvfd                        1/1     Running     0          2d4h\n",
      "openshift-authentication                     oauth-openshift-84d9db6f4f-qrvd6                        1/1     Running     0          2d4h\n",
      "openshift-cluster-machine-approver           machine-approver-5bff865cd7-69qrm                       2/2     Running     0          34d\n",
      "openshift-cluster-node-tuning-operator       cluster-node-tuning-operator-6b96669cd8-6p94r           1/1     Running     0          34d\n",
      "openshift-cluster-node-tuning-operator       tuned-gr7n8                                             1/1     Running     0          35d\n",
      "openshift-cluster-samples-operator           cluster-samples-operator-7689b7755f-7h84t               2/2     Running     0          34d\n",
      "openshift-cluster-version                    cluster-version-operator-7cfdb646d9-td4pt               1/1     Running     0          34d\n",
      "openshift-config-operator                    openshift-config-operator-5bb8c844b5-8hk4j              1/1     Running     0          34d\n",
      "openshift-console-operator                   console-operator-6ff86ff78-pvr4p                        1/1     Running     0          34d\n",
      "openshift-console                            console-8c49bdd8d-bsjcm                                 1/1     Running     0          35d\n",
      "openshift-console                            console-8c49bdd8d-gwq8z                                 1/1     Running     0          35d\n",
      "openshift-console                            downloads-74dd7bffbd-vg8nw                              1/1     Running     0          34d\n",
      "openshift-controller-manager-operator        openshift-controller-manager-operator-d4cdcff59-l75b2   1/1     Running     0          34d\n",
      "openshift-controller-manager                 controller-manager-f6jp4                                1/1     Running     0          2d4h\n",
      "openshift-dns-operator                       dns-operator-68dff9d85c-db4c9                           2/2     Running     0          34d\n",
      "openshift-dns                                dns-default-wlh2p                                       3/3     Running     0          35d\n",
      "openshift-etcd-operator                      etcd-operator-556f5c666d-vsxwh                          1/1     Running     0          34d\n",
      "openshift-etcd                               etcd-crc-pkjt4-master-0                                 3/3     Running     0          35d\n",
      "openshift-etcd                               etcd-quorum-guard-6cc7f5954d-jhcp4                      1/1     Running     0          35d\n",
      "openshift-etcd                               revision-pruner-2-crc-pkjt4-master-0                    0/1     Completed   0          34d\n",
      "openshift-image-registry                     cluster-image-registry-operator-54bbd6db68-slct4        1/1     Running     0          34d\n",
      "openshift-image-registry                     image-registry-f5b6b446b-b652x                          1/1     Running     0          2d4h\n",
      "openshift-image-registry                     node-ca-5mmc7                                           1/1     Running     0          35d\n",
      "openshift-infra                              recycler-for-pv0004                                     0/1     Completed   0          2d3h\n",
      "openshift-infra                              recycler-for-pv0011                                     0/1     Completed   0          2d3h\n",
      "openshift-infra                              recycler-for-pv0014                                     0/1     Completed   0          2d3h\n",
      "openshift-infra                              recycler-for-pv0017                                     0/1     Completed   0          2d2h\n",
      "openshift-infra                              recycler-for-pv0025                                     0/1     Completed   0          2d3h\n",
      "openshift-ingress-canary                     ingress-canary-c74ht                                    1/1     Running     0          35d\n",
      "openshift-ingress-operator                   ingress-operator-76b66dfd8-bvf6f                        2/2     Running     0          34d\n",
      "openshift-ingress                            router-default-6bc9ffb97f-k5g8n                         1/1     Running     1          34d\n",
      "openshift-kube-apiserver-operator            kube-apiserver-operator-cc4fd8bfb-95g99                 1/1     Running     0          34d\n",
      "openshift-kube-apiserver                     installer-8-crc-pkjt4-master-0                          0/1     Completed   0          2d4h\n",
      "openshift-kube-apiserver                     kube-apiserver-crc-pkjt4-master-0                       5/5     Running     0          2d4h\n",
      "openshift-kube-apiserver                     revision-pruner-7-crc-pkjt4-master-0                    0/1     Completed   0          34d\n",
      "openshift-kube-apiserver                     revision-pruner-8-crc-pkjt4-master-0                    0/1     Completed   0          2d4h\n",
      "openshift-kube-controller-manager-operator   kube-controller-manager-operator-7ff5754958-26xck       1/1     Running     0          34d\n",
      "openshift-kube-controller-manager            kube-controller-manager-crc-pkjt4-master-0              4/4     Running     0          35d\n",
      "openshift-kube-controller-manager            revision-pruner-8-crc-pkjt4-master-0                    0/1     Completed   0          34d\n",
      "openshift-kube-scheduler-operator            openshift-kube-scheduler-operator-68995bb689-gx82f      1/1     Running     0          34d\n",
      "openshift-kube-scheduler                     openshift-kube-scheduler-crc-pkjt4-master-0             3/3     Running     0          35d\n",
      "openshift-kube-scheduler                     revision-pruner-7-crc-pkjt4-master-0                    0/1     Completed   0          34d\n",
      "openshift-marketplace                        certified-operators-v9jmj                               1/1     Running     0          43m\n",
      "openshift-marketplace                        community-operators-nbcx4                               1/1     Running     0          2d1h\n",
      "openshift-marketplace                        marketplace-operator-7479fc569f-f4s76                   1/1     Running     0          34d\n",
      "openshift-marketplace                        redhat-marketplace-fc6q8                                1/1     Running     0          43h\n",
      "openshift-marketplace                        redhat-operators-spndt                                  1/1     Running     0          2d1h\n",
      "openshift-multus                             multus-admission-controller-s4wxm                       2/2     Running     0          35d\n",
      "openshift-multus                             multus-lpmch                                            1/1     Running     0          35d\n",
      "openshift-multus                             network-metrics-daemon-wpvml                            2/2     Running     0          35d\n",
      "openshift-network-diagnostics                network-check-source-8d9c49ddc-gjb5c                    1/1     Running     0          35d\n",
      "openshift-network-diagnostics                network-check-target-hbcvg                              1/1     Running     0          35d\n",
      "openshift-network-operator                   network-operator-7dbc7587b7-j4qhk                       1/1     Running     0          34d\n",
      "openshift-oauth-apiserver                    apiserver-77bbbc8db6-6mz4v                              1/1     Running     0          35d\n",
      "openshift-operator-lifecycle-manager         catalog-operator-7dcf5bc6b6-66h5h                       1/1     Running     0          34d\n",
      "openshift-operator-lifecycle-manager         olm-operator-f876cff8c-g6f6q                            1/1     Running     0          34d\n",
      "openshift-operator-lifecycle-manager         packageserver-8477f7555b-zt7b7                          1/1     Running     0          35d\n",
      "openshift-sdn                                sdn-9dsbd                                               2/2     Running     0          35d\n",
      "openshift-sdn                                sdn-controller-rhv9k                                    1/1     Running     0          35d\n",
      "openshift-service-ca-operator                service-ca-operator-74556f96bc-tgn2g                    1/1     Running     0          34d\n",
      "openshift-service-ca                         service-ca-78d76c8f96-b58w8                             1/1     Running     0          35d\n",
      "sftp                                         sftp-client-f4c4d8fc9-khhqn                             1/1     Running     0          43h\n"
     ]
    }
   ],
   "source": [
    "oc exec nginx-unprivileged -- /tmp/kubectl get pods -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project.project.openshift.io \"k8s-sec-2021-07-15\" deleted\n"
     ]
    }
   ],
   "source": [
    "oc delete project $PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning\n",
    "\n",
    "**the rolebinding has survived the deletion of the project/namespace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;31m\u001b[Kcluster-admin-binding\u001b[m\u001b[K                                                       ClusterRole/cluster-admin                                                               2d3h\n",
      "\u001b[01;31m\u001b[Kcluster-admin-binding\u001b[m\u001b[K-k8s-sec-2021-07-13                                    ClusterRole/cluster-admin                                                               2d2h\n",
      "\u001b[01;31m\u001b[Kcluster-admin-binding\u001b[m\u001b[K-k8s-sec-2021-07-15                                    ClusterRole/cluster-admin                                                               2m9s\n"
     ]
    }
   ],
   "source": [
    "oc get clusterrolebinding | grep -E '(^cluster-admin-binding)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
